{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 10, 6])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NUMBER = 11\n",
    "BATCH_SIZE = 12\n",
    "SEQUENCE_LEN = 10\n",
    "\n",
    "# embedding onehot이 0~3 을 랜덤으로 생성한다\n",
    "input_tensor = torch.randint(0, CLASS_NUMBER, (BATCH_SIZE, SEQUENCE_LEN, 6))\n",
    "\n",
    "# tensor size 확인 (batch_size, seq_len, 6)\n",
    "input_tensor.size() # >>> torch.Size([12, 10, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5, 3, 0, 5, 2, 2],\n",
       "         [3, 1, 4, 1, 4, 2],\n",
       "         [0, 7, 2, 6, 6, 5],\n",
       "         [5, 6, 1, 0, 7, 0],\n",
       "         [6, 7, 4, 5, 7, 2],\n",
       "         [2, 0, 7, 0, 1, 4],\n",
       "         [1, 4, 1, 2, 3, 3],\n",
       "         [5, 0, 4, 1, 5, 5],\n",
       "         [0, 4, 6, 7, 4, 3],\n",
       "         [7, 5, 1, 7, 3, 2]],\n",
       "\n",
       "        [[7, 2, 3, 3, 7, 7],\n",
       "         [5, 6, 6, 3, 1, 4],\n",
       "         [1, 1, 6, 6, 2, 0],\n",
       "         [6, 1, 4, 6, 2, 2],\n",
       "         [7, 4, 7, 2, 3, 3],\n",
       "         [0, 3, 1, 3, 0, 0],\n",
       "         [6, 1, 4, 3, 1, 6],\n",
       "         [1, 4, 6, 0, 2, 3],\n",
       "         [4, 5, 0, 2, 7, 7],\n",
       "         [4, 1, 4, 6, 1, 6]],\n",
       "\n",
       "        [[7, 7, 7, 2, 4, 7],\n",
       "         [3, 3, 1, 3, 2, 5],\n",
       "         [7, 4, 1, 1, 7, 7],\n",
       "         [2, 0, 2, 3, 1, 0],\n",
       "         [2, 0, 1, 5, 2, 4],\n",
       "         [1, 6, 6, 7, 3, 1],\n",
       "         [1, 1, 6, 5, 4, 4],\n",
       "         [0, 2, 0, 5, 0, 0],\n",
       "         [7, 1, 4, 2, 4, 7],\n",
       "         [4, 5, 6, 3, 1, 5]],\n",
       "\n",
       "        [[6, 3, 5, 5, 6, 6],\n",
       "         [3, 1, 1, 5, 4, 7],\n",
       "         [6, 0, 3, 6, 0, 3],\n",
       "         [2, 3, 2, 0, 7, 5],\n",
       "         [4, 7, 4, 2, 0, 6],\n",
       "         [0, 7, 2, 1, 7, 1],\n",
       "         [7, 5, 4, 1, 7, 3],\n",
       "         [3, 2, 0, 7, 4, 7],\n",
       "         [0, 6, 4, 7, 1, 4],\n",
       "         [2, 2, 5, 5, 0, 6]],\n",
       "\n",
       "        [[0, 0, 7, 4, 3, 1],\n",
       "         [7, 0, 7, 6, 7, 2],\n",
       "         [1, 4, 2, 2, 0, 4],\n",
       "         [2, 4, 2, 5, 3, 7],\n",
       "         [5, 6, 5, 3, 4, 3],\n",
       "         [5, 3, 0, 3, 7, 6],\n",
       "         [1, 5, 6, 7, 7, 6],\n",
       "         [4, 6, 1, 1, 1, 1],\n",
       "         [6, 0, 7, 6, 4, 3],\n",
       "         [1, 0, 3, 2, 5, 3]],\n",
       "\n",
       "        [[0, 4, 3, 7, 4, 2],\n",
       "         [2, 7, 6, 3, 5, 0],\n",
       "         [4, 2, 1, 7, 6, 0],\n",
       "         [5, 4, 7, 6, 5, 0],\n",
       "         [7, 0, 1, 6, 7, 1],\n",
       "         [1, 4, 4, 7, 6, 5],\n",
       "         [2, 3, 5, 4, 3, 6],\n",
       "         [2, 4, 7, 2, 0, 5],\n",
       "         [6, 6, 4, 0, 4, 7],\n",
       "         [0, 7, 2, 3, 6, 5]],\n",
       "\n",
       "        [[7, 6, 4, 5, 1, 0],\n",
       "         [4, 2, 0, 6, 4, 4],\n",
       "         [2, 3, 7, 1, 7, 2],\n",
       "         [2, 1, 5, 1, 1, 5],\n",
       "         [5, 4, 1, 0, 0, 5],\n",
       "         [1, 3, 4, 7, 4, 0],\n",
       "         [6, 6, 3, 1, 6, 7],\n",
       "         [0, 0, 3, 7, 2, 6],\n",
       "         [7, 5, 2, 1, 5, 7],\n",
       "         [4, 6, 2, 0, 2, 6]],\n",
       "\n",
       "        [[0, 4, 5, 2, 6, 3],\n",
       "         [7, 5, 3, 0, 4, 1],\n",
       "         [1, 6, 4, 5, 4, 3],\n",
       "         [6, 3, 2, 5, 0, 2],\n",
       "         [3, 3, 3, 4, 6, 5],\n",
       "         [7, 6, 2, 1, 4, 0],\n",
       "         [1, 5, 4, 6, 2, 7],\n",
       "         [6, 6, 3, 5, 6, 2],\n",
       "         [7, 3, 4, 2, 3, 4],\n",
       "         [4, 6, 0, 0, 4, 5]],\n",
       "\n",
       "        [[3, 1, 3, 3, 2, 4],\n",
       "         [1, 2, 7, 4, 7, 0],\n",
       "         [3, 5, 5, 6, 2, 4],\n",
       "         [3, 1, 6, 2, 2, 5],\n",
       "         [7, 1, 3, 5, 6, 3],\n",
       "         [2, 7, 3, 6, 4, 7],\n",
       "         [6, 1, 7, 5, 1, 4],\n",
       "         [6, 0, 4, 0, 4, 2],\n",
       "         [4, 4, 0, 7, 6, 3],\n",
       "         [6, 4, 5, 7, 6, 3]],\n",
       "\n",
       "        [[6, 7, 5, 2, 4, 7],\n",
       "         [6, 0, 3, 4, 2, 0],\n",
       "         [2, 1, 3, 6, 6, 1],\n",
       "         [1, 1, 2, 2, 0, 1],\n",
       "         [5, 3, 1, 5, 1, 7],\n",
       "         [6, 1, 1, 3, 1, 6],\n",
       "         [0, 4, 1, 3, 6, 1],\n",
       "         [4, 0, 7, 3, 1, 6],\n",
       "         [4, 5, 0, 2, 5, 2],\n",
       "         [5, 5, 2, 5, 1, 6]],\n",
       "\n",
       "        [[4, 2, 0, 3, 4, 3],\n",
       "         [6, 6, 0, 1, 7, 2],\n",
       "         [7, 1, 7, 2, 7, 0],\n",
       "         [4, 7, 1, 7, 4, 1],\n",
       "         [3, 0, 3, 7, 1, 0],\n",
       "         [2, 6, 1, 4, 3, 5],\n",
       "         [5, 4, 7, 7, 2, 6],\n",
       "         [3, 0, 5, 7, 3, 2],\n",
       "         [1, 3, 0, 4, 4, 2],\n",
       "         [2, 3, 2, 5, 3, 5]],\n",
       "\n",
       "        [[5, 3, 5, 3, 4, 1],\n",
       "         [7, 4, 2, 6, 3, 3],\n",
       "         [6, 7, 3, 2, 5, 0],\n",
       "         [2, 5, 4, 4, 0, 2],\n",
       "         [4, 3, 3, 4, 1, 0],\n",
       "         [4, 3, 0, 5, 4, 6],\n",
       "         [1, 1, 3, 1, 5, 2],\n",
       "         [7, 0, 1, 4, 0, 0],\n",
       "         [3, 6, 4, 2, 5, 7],\n",
       "         [0, 3, 3, 4, 6, 2]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 10, 6, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding 선언 (embedding max값, embed_dim)\n",
    "embedding = torch.nn.Embedding(CLASS_NUMBER, 100)\n",
    "\n",
    "# embedding 생성\n",
    "embedding_tensor = embedding(input_tensor)\n",
    "# tensor size 확인 (batch_size, seq_len, 6, embed_dim)\n",
    "embedding_tensor.size() # >>> torch.Size([12, 10, 6, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]],\n",
       "\n",
       "         [[-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]],\n",
       "\n",
       "         [[ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838]],\n",
       "\n",
       "         [[ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142]],\n",
       "\n",
       "         [[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         [[-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454]],\n",
       "\n",
       "         [[ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142]],\n",
       "\n",
       "         [[ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         [[ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         [[-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838]],\n",
       "\n",
       "         [[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671]],\n",
       "\n",
       "         [[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         [[ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         [[ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671]],\n",
       "\n",
       "         [[ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544]],\n",
       "\n",
       "         [[ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]],\n",
       "\n",
       "         [[-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142]],\n",
       "\n",
       "         [[ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]],\n",
       "\n",
       "         [[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]],\n",
       "\n",
       "         [[ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]],\n",
       "\n",
       "         [[ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838]]],\n",
       "\n",
       "\n",
       "        [[[-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176]],\n",
       "\n",
       "         [[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142]],\n",
       "\n",
       "         [[ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 1.0668,  0.1609, -0.4067,  ..., -1.3819, -1.7167, -0.2176],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671]],\n",
       "\n",
       "         [[-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108],\n",
       "          [-1.0663, -0.3826, -0.8024,  ...,  0.1620, -1.5806, -0.6838],\n",
       "          [ 0.7456,  0.0187,  0.7457,  ..., -0.0423, -0.3876,  0.1602]],\n",
       "\n",
       "         [[ 0.1181,  1.6771,  0.5077,  ..., -0.5421, -1.7466, -0.5671],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [-0.8935,  0.4409,  0.2324,  ...,  0.0743, -0.8539, -1.2142],\n",
       "          [ 0.4234, -0.0327,  1.7739,  ..., -0.4352,  0.5715, -0.8454],\n",
       "          [ 0.6642,  1.4890,  0.2772,  ...,  0.6748,  0.1192,  0.4544],\n",
       "          [ 0.7787,  1.3764, -1.5493,  ..., -1.2452,  1.0228,  0.8108]]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_length,\n",
    "            hidden_size,\n",
    "            output_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        print(input.shape)\n",
    "        h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "        c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "        print(h_0.shape, c_0.shape)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        print(output.shape)\n",
    "        output = self.linear(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (lstm): LSTM(100, 128)\n",
       "  (linear): Linear(in_features=128, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleLSTM(embedding_length=100, hidden_size=128, output_size=CLASS_NUMBER)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 10, 6])\n",
      "torch.Size([1, 12, 128]) torch.Size([1, 12, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 100, got 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\joona\\Documents\\Git Repos\\gyro-rnn-test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m, in \u001b[0;36mSimpleLSTM.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     24\u001b[0m c_0 \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(h_0\u001b[39m.\u001b[39mshape, c_0\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 26\u001b[0m output, (final_hidden_state, final_cell_state) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, (h_0, c_0))\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     28\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(output)\n",
      "File \u001b[1;32mc:\\Users\\joona\\Documents\\Git Repos\\gyro-rnn-test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joona\\Documents\\Git Repos\\gyro-rnn-test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\joona\\Documents\\Git Repos\\gyro-rnn-test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joona\\Documents\\Git Repos\\gyro-rnn-test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 100, got 6"
     ]
    }
   ],
   "source": [
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1000, 6])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "N = 1000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "input_tensor = torch.from_numpy(np.random.randn(BATCH_SIZE, N, 6))\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3334,  0.0900,  0.9695,  2.2554, -0.8541, -0.3263],\n",
       "         [ 0.5470, -0.1725, -2.0678,  1.0781, -0.9950,  0.5495],\n",
       "         [ 0.5056,  0.4921,  0.6030,  1.9122, -0.9606,  0.1736],\n",
       "         ...,\n",
       "         [-1.4324,  0.7545,  0.2106, -0.2328, -0.1378, -0.4016],\n",
       "         [-1.0029, -0.0336, -0.6210, -0.7797,  1.4970, -1.3926],\n",
       "         [ 1.1494,  0.7634, -1.2224, -0.7693, -0.0620, -0.9935]],\n",
       "\n",
       "        [[ 0.1316, -1.4035,  0.7420,  1.2231, -2.1166,  0.5537],\n",
       "         [ 1.1525, -0.2788, -0.9909, -0.0120, -0.1188, -0.5925],\n",
       "         [-0.2347, -0.8860, -0.7605, -0.7242,  0.6367,  0.3241],\n",
       "         ...,\n",
       "         [ 0.2209,  0.6896,  0.7259, -0.8754, -0.8024, -0.0131],\n",
       "         [-0.4730, -0.4438, -1.2718, -1.1187,  0.5256, -1.0091],\n",
       "         [-1.3298,  0.2243, -0.2588,  0.6377,  1.8225, -0.4287]],\n",
       "\n",
       "        [[ 0.7548,  0.4390,  2.0778, -1.1392, -0.4453,  0.0216],\n",
       "         [-0.3449,  0.0689, -0.2040, -0.0097,  0.9132,  0.1264],\n",
       "         [-0.3837,  0.2758, -0.3398,  0.4550, -0.0361, -0.6950],\n",
       "         ...,\n",
       "         [ 0.9275,  0.2152, -0.9760, -0.2316, -0.2467, -0.4179],\n",
       "         [-0.3187,  1.1897,  0.4842, -1.5435, -0.9373, -1.3261],\n",
       "         [-0.3748, -1.8136,  0.9584, -0.8507, -0.1345,  0.7659]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2744, -0.2504,  0.6587,  0.2343, -1.3725, -0.3372],\n",
       "         [-2.3776, -0.2065,  0.5326,  0.7992,  1.3747,  0.5005],\n",
       "         [-1.2381,  0.3435, -0.6901, -0.5898, -0.0934, -0.2315],\n",
       "         ...,\n",
       "         [ 1.0035, -0.9945, -1.0549,  0.1943, -0.6183, -0.2710],\n",
       "         [ 1.6060,  0.4202, -1.4247, -0.0918,  0.5273, -0.3907],\n",
       "         [-1.0944,  0.2437, -0.5456, -1.0095, -1.1722, -0.5637]],\n",
       "\n",
       "        [[ 0.0145,  0.2513,  0.5795,  1.7460,  0.9073,  0.4842],\n",
       "         [ 1.1565,  0.1112, -1.3950, -0.2140, -0.7567,  0.4882],\n",
       "         [-1.4099,  0.0212, -0.1921, -1.7462, -0.0427, -1.5767],\n",
       "         ...,\n",
       "         [-0.5395,  1.2025,  1.1403,  1.0326, -1.2888,  0.9827],\n",
       "         [-0.5178, -0.2919, -1.3449, -0.2593,  0.6214, -1.2865],\n",
       "         [-0.2123,  0.5259,  1.5736, -0.9073,  0.2539, -0.2291]],\n",
       "\n",
       "        [[ 1.2193, -2.0356,  0.0175, -0.1418, -0.1541, -0.6900],\n",
       "         [ 0.5881,  1.0849,  0.6888, -1.1211,  0.0429,  1.4122],\n",
       "         [ 1.3853,  0.6454, -0.9737, -0.3149, -0.0741,  0.7473],\n",
       "         ...,\n",
       "         [-0.2072, -0.2028, -0.1403, -0.3360,  0.0483, -0.1042],\n",
       "         [ 0.4430,  1.0162,  0.0986,  0.6835, -0.5579, -1.1016],\n",
       "         [ 0.1150, -0.0516, -1.5645,  1.1494, -2.2552, -0.8515]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1000, 6])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0882, -0.0888, -0.1193,  ...,  0.1104,  0.3252, -0.0288],\n",
       "         [ 0.0032, -0.0716,  0.2534,  ..., -0.0874, -0.1426, -0.1064],\n",
       "         [ 0.0012, -0.0754, -0.0522,  ...,  0.0983,  0.2599, -0.0454],\n",
       "         ...,\n",
       "         [-0.0395, -0.0619,  0.0749,  ..., -0.0808, -0.0118,  0.1038],\n",
       "         [ 0.1853, -0.1070,  0.0817,  ..., -0.2306, -0.0620,  0.1535],\n",
       "         [ 0.0720, -0.1495,  0.2136,  ..., -0.0501,  0.0146, -0.0718]],\n",
       "\n",
       "        [[ 0.0119,  0.0124, -0.2695,  ...,  0.1151,  0.2413, -0.0732],\n",
       "         [ 0.1060, -0.1177,  0.2300,  ..., -0.1368, -0.1111, -0.1700],\n",
       "         [ 0.0545, -0.0724,  0.1009,  ..., -0.1397,  0.0062, -0.1581],\n",
       "         ...,\n",
       "         [-0.0619,  0.0419,  0.1598,  ..., -0.0368,  0.1139,  0.0903],\n",
       "         [ 0.1696, -0.1375,  0.1910,  ..., -0.2479, -0.1425,  0.1355],\n",
       "         [ 0.2747, -0.2192,  0.2524,  ..., -0.2561, -0.0686, -0.0575]],\n",
       "\n",
       "        [[-0.0307,  0.1661, -0.0526,  ...,  0.1689,  0.2001,  0.0195],\n",
       "         [ 0.1757, -0.1866,  0.2663,  ..., -0.1917, -0.0561, -0.1550],\n",
       "         [ 0.0960, -0.1514,  0.1368,  ..., -0.1622, -0.0516, -0.1078],\n",
       "         ...,\n",
       "         [ 0.0035, -0.0830,  0.2577,  ..., -0.0861,  0.1490, -0.0539],\n",
       "         [ 0.0418, -0.1536,  0.2120,  ..., -0.1533, -0.0738,  0.2461],\n",
       "         [ 0.0875, -0.0262,  0.0656,  ..., -0.2731, -0.0103, -0.0396]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0089,  0.0810, -0.1056,  ..., -0.0313,  0.1659, -0.0333],\n",
       "         [ 0.1398, -0.1247,  0.0475,  ..., -0.3622, -0.0720,  0.0539],\n",
       "         [ 0.0953, -0.1901,  0.2313,  ..., -0.2475, -0.0926,  0.0192],\n",
       "         ...,\n",
       "         [ 0.1386, -0.0789,  0.1542,  ..., -0.0870,  0.2604, -0.1916],\n",
       "         [ 0.0741, -0.0589,  0.2549,  ..., -0.0362,  0.4176, -0.1922],\n",
       "         [ 0.0202, -0.0717,  0.3555,  ..., -0.2003, -0.0701, -0.1314]],\n",
       "\n",
       "        [[ 0.1268, -0.0573,  0.0040,  ..., -0.0031,  0.2019, -0.0892],\n",
       "         [ 0.0821, -0.0961,  0.2929,  ..., -0.1063, -0.0657, -0.0894],\n",
       "         [ 0.0711, -0.1727,  0.1639,  ..., -0.3398, -0.0753,  0.2979],\n",
       "         ...,\n",
       "         [-0.0109, -0.0180,  0.0943,  ...,  0.1153,  0.3541, -0.1186],\n",
       "         [ 0.1559, -0.1238,  0.2865,  ..., -0.2182,  0.0767, -0.2544],\n",
       "         [ 0.0487, -0.0028,  0.1724,  ..., -0.1451,  0.0435,  0.0471]],\n",
       "\n",
       "        [[ 0.1330, -0.0372, -0.0443,  ..., -0.1505,  0.0372, -0.1597],\n",
       "         [ 0.0194,  0.0333,  0.2963,  ..., -0.0464,  0.1426, -0.1054],\n",
       "         [ 0.0794, -0.1500,  0.3214,  ..., -0.0991,  0.1107, -0.0142],\n",
       "         ...,\n",
       "         [ 0.0179, -0.0543,  0.1187,  ..., -0.0450,  0.1589, -0.1498],\n",
       "         [ 0.1431, -0.2341,  0.2337,  ..., -0.0727,  0.1388, -0.1389],\n",
       "         [-0.0021, -0.0734,  0.1457,  ..., -0.1292, -0.1230, -0.0607]]],\n",
       "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_DIM = 8\n",
    "\n",
    "input_tensor = input_tensor.float().cuda()\n",
    "\n",
    "lstm = nn.LSTM(input_size=6, hidden_size=HIDDEN_DIM).cuda()\n",
    "\n",
    "h_0 = Variable(torch.zeros(1, N, HIDDEN_DIM).cuda())\n",
    "c_0 = Variable(torch.zeros(1, N, HIDDEN_DIM).cuda())\n",
    "\n",
    "output, (final_hidden_state, final_cell_state) = lstm(input_tensor, (h_0, c_0))\n",
    "\n",
    "output # (batch_size, N, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1000, 8])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 8])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=8, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_FC_DIM = 128\n",
    "OUTPUT_CLASSES = 6\n",
    "\n",
    "fc1 = nn.Linear(in_features=HIDDEN_DIM, out_features=HIDDEN_FC_DIM, device='cuda:0')\n",
    "fc2 = nn.Linear(in_features=HIDDEN_FC_DIM, out_features=OUTPUT_CLASSES, device='cuda:0')\n",
    "\n",
    "print(fc1)\n",
    "print(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1938,  0.0631,  0.1251,  0.0869,  0.0924, -0.1401],\n",
       "         [-0.2406,  0.1020,  0.0793,  0.1855,  0.0570, -0.0394],\n",
       "         [-0.2544,  0.1122,  0.0844,  0.1206, -0.0222, -0.0503],\n",
       "         ...,\n",
       "         [-0.2476,  0.1166,  0.0931,  0.1367,  0.0669, -0.0792],\n",
       "         [-0.2534,  0.1092,  0.1231,  0.1111,  0.0137, -0.0773],\n",
       "         [-0.2388,  0.1135,  0.0777,  0.0919,  0.0107, -0.1263]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fc1 = fc1(final_hidden_state.cuda())\n",
    "output_fc2 = fc2(output_fc1)\n",
    "output_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 6])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1938,  0.0631,  0.1251,  0.0869,  0.0924, -0.1401],\n",
       "        [-0.2406,  0.1020,  0.0793,  0.1855,  0.0570, -0.0394],\n",
       "        [-0.2544,  0.1122,  0.0844,  0.1206, -0.0222, -0.0503],\n",
       "        ...,\n",
       "        [-0.2476,  0.1166,  0.0931,  0.1367,  0.0669, -0.0792],\n",
       "        [-0.2534,  0.1092,  0.1231,  0.1111,  0.0137, -0.0773],\n",
       "        [-0.2388,  0.1135,  0.0777,  0.0919,  0.0107, -0.1263]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joona\\AppData\\Local\\Temp\\ipykernel_40856\\1574591000.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(output_fc2[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1355, 0.1752, 0.1864, 0.1794, 0.1804, 0.1430],\n",
       "        [0.1268, 0.1786, 0.1746, 0.1942, 0.1707, 0.1551],\n",
       "        [0.1284, 0.1852, 0.1802, 0.1868, 0.1619, 0.1575],\n",
       "        ...,\n",
       "        [0.1271, 0.1830, 0.1787, 0.1867, 0.1741, 0.1504],\n",
       "        [0.1277, 0.1835, 0.1860, 0.1838, 0.1668, 0.1523],\n",
       "        [0.1318, 0.1874, 0.1808, 0.1834, 0.1691, 0.1475]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.softmax(output_fc2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13551765, 0.17521291, 0.18641994, 0.179433  , 0.1804226 ,\n",
       "       0.14299389])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax\n",
    "odds = np.exp([-0.1938,  0.0631,  0.1251,  0.0869,  0.0924, -0.1401])\n",
    "odds / sum(odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999900000001"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.13551765, 0.17521291, 0.18641994, 0.179433  , 0.1804226 , 0.14299389])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 3, 3, 2, 3, 2, 3, 3, 1, 3, 2, 3, 2, 1, 3, 3, 2, 2, 3, 2, 1, 2, 2,\n",
       "        3, 2, 2, 1, 3, 3, 2, 1, 3, 3, 2, 2, 3, 3, 1, 3, 2, 3, 1, 3, 1, 1, 1, 2,\n",
       "        2, 2, 2, 2, 2, 2, 3, 3, 1, 2, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1, 4, 2, 2,\n",
       "        3, 3, 1, 3, 2, 1, 1, 1, 1, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 1, 3, 2, 1, 3,\n",
       "        3, 2, 1, 3, 1, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3,\n",
       "        2, 2, 1, 2, 1, 3, 2, 2, 2, 3, 1, 1, 3, 3, 2, 2, 3, 1, 3, 3, 2, 3, 1, 2,\n",
       "        2, 3, 1, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 1, 2, 2, 3, 2, 3,\n",
       "        2, 2, 3, 2, 2, 3, 3, 1, 1, 2, 2, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 3,\n",
       "        2, 3, 2, 2, 3, 1, 3, 2, 2, 1, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 1, 3, 2,\n",
       "        3, 3, 2, 3, 2, 3, 1, 2, 3, 3, 3, 2, 1, 3, 2, 3, 3, 2, 1, 2, 3, 2, 3, 3,\n",
       "        1, 3, 2, 3, 1, 3, 3, 3, 3, 3, 2, 3, 1, 2, 2, 3, 2, 2, 3, 2, 1, 1, 3, 2,\n",
       "        3, 2, 1, 2, 3, 3, 2, 3, 2, 2, 2, 3, 1, 3, 3, 1, 3, 2, 3, 3, 2, 1, 2, 2,\n",
       "        2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 2, 1, 2, 2, 3, 3, 3, 3, 3, 1, 3, 3, 2, 2,\n",
       "        3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 1, 1, 1, 2, 2,\n",
       "        2, 3, 3, 1, 1, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 1, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "        2, 3, 3, 2, 2, 1, 2, 3, 1, 3, 2, 2, 3, 1, 3, 3, 2, 3, 2, 2, 3, 1, 3, 2,\n",
       "        2, 2, 3, 2, 1, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 1, 1, 3, 2, 1, 3, 3, 2,\n",
       "        1, 2, 3, 3, 2, 3, 2, 1, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 1, 2, 2, 3, 3,\n",
       "        2, 3, 1, 2, 3, 2, 1, 2, 3, 2, 1, 3, 2, 1, 3, 1, 3, 3, 2, 2, 2, 1, 1, 3,\n",
       "        2, 2, 2, 1, 3, 2, 1, 2, 2, 1, 1, 2, 2, 2, 3, 3, 2, 3, 1, 3, 3, 3, 3, 2,\n",
       "        3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 2, 3, 2, 1, 3, 2, 3, 3, 3,\n",
       "        2, 3, 2, 2, 1, 3, 2, 3, 1, 3, 3, 2, 2, 2, 1, 2, 1, 2, 3, 2, 2, 2, 2, 2,\n",
       "        3, 3, 3, 3, 1, 2, 1, 2, 2, 1, 2, 3, 1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3,\n",
       "        2, 3, 1, 3, 2, 3, 3, 1, 2, 2, 2, 2, 2, 1, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2,\n",
       "        1, 2, 2, 3, 2, 2, 1, 2, 1, 3, 3, 2, 3, 3, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1,\n",
       "        2, 2, 2, 3, 2, 3, 1, 3, 3, 2, 2, 2, 2, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 2,\n",
       "        1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 1, 2, 1, 2, 2, 3, 2, 1, 2, 1, 2,\n",
       "        1, 1, 3, 2, 2, 1, 1, 2, 3, 3, 1, 3, 3, 3, 2, 2, 2, 1, 2, 3, 3, 3, 1, 2,\n",
       "        2, 2, 2, 1, 1, 3, 3, 2, 3, 3, 2, 3, 1, 1, 2, 3, 2, 3, 3, 1, 1, 1, 1, 1,\n",
       "        2, 1, 2, 3, 3, 3, 2, 2, 2, 3, 1, 2, 3, 3, 1, 3, 2, 2, 3, 3, 3, 2, 1, 1,\n",
       "        2, 1, 2, 2, 3, 2, 3, 2, 3, 1, 3, 3, 3, 2, 2, 2, 1, 2, 3, 2, 3, 3, 2, 3,\n",
       "        1, 2, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 1, 3, 1, 2, 2, 3, 3, 3, 2, 3, 2,\n",
       "        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1,\n",
       "        3, 2, 2, 1, 3, 3, 3, 1, 2, 2, 2, 2, 3, 3, 2, 2, 2, 1, 1, 3, 2, 2, 1, 2,\n",
       "        3, 2, 2, 1, 3, 2, 1, 3, 2, 2, 3, 3, 1, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2,\n",
       "        3, 2, 3, 3, 3, 2, 3, 2, 3, 1, 2, 3, 2, 3, 3, 3, 3, 2, 3, 1, 2, 2, 1, 3,\n",
       "        1, 2, 1, 3, 1, 2, 1, 1, 1, 3, 2, 2, 3, 3, 3, 1, 2, 1, 1, 1, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 2, 1, 3, 1, 3, 1, 3, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 3, 1, 3,\n",
       "        3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 2, 2, 2, 1, 3, 3,\n",
       "        3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 1, 3, 3, 2, 2, 2, 3, 2, 2, 1, 3, 3, 2, 3,\n",
       "        2, 1, 3, 3, 1, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 1, 3, 3, 3,\n",
       "        2, 1, 3, 2, 2, 1, 3, 1, 2, 2, 2, 3, 2, 3, 2, 1], device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output_fc2[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
